
===============================================================================
= Video Delivery Server: User Guide ===========================================
===============================================================================

Note this guide is written and intended for use by programmers/system administrators, who will be responsible for installing, running, and maintaining the system.



= Software requirements =======================================================
 - Linux (Ubuntu 9.10)
   http://www.ubuntu.com/
 - Python 2.6
   http://www.python.org/
 - MySQL-python 1.2.3c1
   http://sourceforge.net/projects/mysql-python/
 - FFmpeg (latest version) includes FFserver
   http://www.ffmpeg.org/
 - Apache HTTP Server 2.2 (for archive videos and thumbnails)
   http://httpd.apache.org/



= Hardware requirements =======================================================
 - As much RAM, CPU, and Bandwidth as you can get :)



= Installation ================================================================
 1) Install Linux, Python, MySQL-python, FFmpeg/FFserver, and Apache.
    Default configurations are fine.
 2) Checkout latest code from SVN
    http://code.google.com/p/magiclegoblimps/
    You can put this anywhere you feel like.
 3) Edit settings file (details below)
    magiclegoblimps/Server/VideoDelivery/settings.py
 4) Wait for client database and the QoS server to start
 5) Launch Video Delivery server
    python vidcontrol.py
 6) Press ctrl-c to shutdown server
    The program will catch this signal and perform all of the necessary
    cleanup.
 7) If everything goes wrong:
    -- search for and kill all ffserver/ffmpeg processes
    -- delete all ffserver-*.conf files
    -- check to logs to try and figure out what happened



= Settings (settings.py) ======================================================

This is the most important thing. The settings.py files is a normal Python file (so any Python code is valid) used to configure the server. Here is a description of all the settings and how to configure them. Read this section carefully. This list is not comprehensive. Some settings are being phased out, and not all are essential to program operation. A basic familiarity with both Python and the Linux filesystem are necessary to correctly edit this file.

DEBUG - Whether or not the project in DEBUG mode. This should be set to False for a production environment, or True for testing. When in debug mode, the program uses dummy programs that do not do anything in place of launching ffmpeg/ffserver processes for transcoding/streaming video.

ROOT_DIR - A list of directories in which the program will run. It chooses the first one that exists on your system, and defaults to the current directory if all fail. This allows us to accomodate different installation paths in different testing environments.

LOGFILE - Relative path to the server's runtime log. You should create all directories listed in this path before the program runs.

TRANSCRIPT_FILE - Relative path to the log that records all of the data sent between the video delivery server and the quality of service (QoS) server.

QOS_SERVER_URL - The host name or IP address for the QoS server.

QOS_SERVER_PORT - The port on which to connect to the QoS server.

MAX_CONNECTION_ATTEMPTS - Set the number of times to retry a connection to the QoS server if the initial connection fails.

CONNECTION_RETRY_INTERVAL - The time in seconds between connection attempts.

CURRENT_IP - The current host name or IP address for the video delivery server. This can be obtained by running the ifconfig command.

FEEDER_URLS - Format of the URLs used internally by FFserver during transcoding. Don't modify this unless you know what you're doing.

LIVE_FEED_URLS - Format of the URLs for the live Flash video streams. Don't modify this unless you know what you're doing.

ARCHIVE_FEED_URLS - The base URL including directory path in which the archive videos and thumbnails are stored. This URL should correspond to the physical directory specified in the ARCHIVE_DIR setting.

ARCHIVE_CSV_FILE - Deprecated. We are instead updating the client group's database directly.

ARCHIVE_HTML - Testing only. An HTML file that lists links to all of the archive videos.

ARCHIVE_DURATION - The amount of time (in seconds) to record for archive videos.

ARCHIVE_QOS_THRESHOLD - If the QoS metric for an object in a camera feed exceeds this value [0.0-1.0], an archive video will be recorded.

ARCHIVE_THUMB_SIZE = The thumbnail size in pixels for archive video thumnails. The format is a python string such as 'WxH', where W is the width in pixels and H is the height in pixels.

CONFIG_TEMPLATE - The template file used to create FFserver configuration files. Don't mess with this unless you're an FFserver guru. You can even modify the contents of the configuration template file if you really, really know FFserver.

CONFIG_FILE_DIR - The directory in which the CONFIG_TEMPLATE file is located.

ARCHIVE_DIR - The directory in which the archive videos and thumbnails are stored. This should be in a web accessible directory (i.e. beneath Apache's server root directory), and the owner under which the video delivery process is executing needs to have permissions to write to this directory. The easiest way to do this is change the owner of the archive directory to be the same as the user running the video delivery server.

CONFIG_NAME - format for FFserver configuration files. Leave this alone.

STREAM_NAME - Used to name/identify live Flash streams. Leave this alone.

FEED_NAME - Used internally by FFserver to track streams being transcoded.

FFMPEG_ARGS - Specify arguments for transcoding video streams. Leave this alone  unless you are familiar with FFmpeg. This is a Python list containing the same arguments you would give on the command line. The input-stream and output-stream arguments will be replaced by the actual input/output streams determined by the video delivery server.

FFSERVER_ARGS - Specify arguments for running FFserver instances. Don't mess with this. FFserver doesn't have any other arguments anyways.

MySQL_HOST - The host name of the client group's MySQL database.

MySQL_USER - The user name for the client group's MySQL database.

MySQL_PASS - The password for the client group's MySQL database.

MySQL_DATABASE - The database for name the client group's MySQL database.



= Log files ===================================================================

You can view the logs to verify everything is working correctly or to diagnose a problem. There are two log files, vidserver.log and transcript.log.
    -- vidserver.log
       This is what's going on inside me? Errors, 
    -- transcript.log
       Exact transcript of all data sent to/from the QoS server labeled
       accordingly.



= Troubleshooting =============================================================

1) Check the log files, they will contain error messages to help diagnose the problem.
2) Help! There are no log files.
    a) Check your settings.py file and try again
    b) Make sure you are connected to the internet and that the client database is running. The MySQL-python driver connection object doesn't automatically timeout, so if one of the two conditions occur, the process will hang. The only way to terminate the program is to kill the process from the shell.
    
= Design =======================================================================

We thought this would be an easy part of the project. We assumed we could find and configure an acceptable open source video server and then move on to other parts of the project. This assumption turned out to be wrong. We required a video server that supported live streaming of Adobe's Flash video format (FLV). Unfortunately, many open source servers did not support this. We found two open source products and two commercial products that claimed to support live FLV streaming. We ruled out the commercial products because they were prohibitively expensive, so that left us with the Red5 media server and FFserver, which is part of the FFmpeg package for video encoding and editing. There were no install instructions for Red5 and no documentation for how to configure and run it either. We managed to get it installed and playing something, but it depended on every Java library known to man and we had no idea where to even begin configuring. After much trial and error with FFserver, we finally managed to modify a sample FFserver config file to produce take a camera feed as input and produce an FLV stream as output.

Once we figured out the FFserver was able to produce an FLV stream from the camera feed as input we had to figure out how this would work for multiple feeds and make sure that this can all be done dynamically. It turns out that in order for us to use FFserver we would have to spawn many process of FFmpeg(this is what does the actual encoding) and FFserver(this is what does the streaming of the encoded video). This means that we would need someway to manage all of the processes so that they spawn accordingly and also killed themselves to make sure a mess of processes isn't left behind. We used knew that python had an easy way of keeping track of these processes and thus decided that it would be benefical to use it. The only other task to tackle was the saving of archived footage in order to show footage in times when the live feed was not on. FFmpeg could encode the video and output it to a stream like we were using or output this to a file which is perfect for achieving. With all of these things taken care of we designed our project in python using the open source FFmpeg and FFserver. 

Since FFserver was the only piece of software we could actually get to stream video, we decided to use it. However, we had to work around some serious shortcomings. Mainly, FFserver can only be statically configured. Any changes, such as adding a new camera feed or removing an existing camera feed, required us to stop the server, modify the config file, then start the server again. Since extensibility was a requirement for this project, this was not acceptable. 

Our workaround for this issue was to have a program that receives messages cameras that need to be added to or removed from the system, and use this program to generate appropriate FFserver config files then launch and kill individual instances of FFserver as needed. Since we were also transcoding the video stream from the MJPEG format to the FLV format, it was necessary to launch appropriately configured FFmpeg instances to perform the conversion. The only issue we faced was that each instance of FFserver must be bound to a separate port, so our program needed to account for this.

So now we have a program that receives messages telling it which camera feeds to add or remove, and the program generates FFserver config files then it launches or kills the appropriate FFserver and FFmpeg processes for those camera feeds. As long as we can access a camera feed, we could transcode it to FLV and stream it to anywhere in the UTD network. The only other concern would be that of generating more processes than the computer could handle, but due to the processor intensive nature of video transcoding and the bandwidth intensive nature of video streaming, we determined that we would run into problems with CPU usage or bandwidth utilization before hitting the process limit for the operating system. For the demo this design was sufficient, but for a production application we would want to extend the design to balance the load across a cluster of similarly configured servers.

As far as performance goes, we were able to transcode and stream several streams at about a 50% CPU usage on a 3GHz Intel dual core processor, and up to about 3Gb/s upload bandwidth peak (wow, I know)! Before committing to our design and the use of FFserver, we were transcoding video from four streams to 17 computers in the lab without difficult. There was about a 3 second delay between an action occuring in real life to the action appearing on the computer, but since it was not critical for our part of the system operate in real time, this delay was acceptable. However, this test was done at around 2 AM when the university's network is under a light load. When we did our first demo around 2 PM, the university's network was under a heavy load, so there were often delays in the video streaming resulting in abrupt pauses in the video as the video player buffered enough of the stream to playback. However, we determined it was out of our control as far as the speed and congestion of the school's network - at least as far as the scope of this project was concerned.

Anyways, since we were already receiving information about the camera feeds from the Quality of Service server (QoS), we decided it would be a nice feature to record archive footage when objects viewable in a camera feed exceeded a certain quality threshold. This would allow users of the system to view high quality footage of particular objects even if those objects are not in view of any camera. Because we do not know when to expect updates form the QoS server, they could be milliseconds apart or minutes apart, we decided to save 30 second clips of archive footage whenever an object exceeded a given QoS threshold and also take a thumbnail of a frame in the archive clip for users to preview before viewing the clip. For this task, our program would spawn additional FFmpeg processes to capture both the archive video clip and the preview thumbnail. One feature missing that would likely be required in a practical implementation would be to delete old archive footage periodically, or replace old archive footage with newer or higher quality footage to avoid filling the entire hard drive with video files.

The last component of our system involves writing information about the live stream URLs and the archive clip and thumbnail URLs to the client group's MySQL databse. This would allow the client group to actually show the video to the system's end users. This was easy to add to our system since we already had the required information we needed to save to the database.

= Interfaces ===================================================================

Input:

The QoS server communicates with the video delivery server using a server push protocal. Whenever the QoS server has new information to share with the video delivery server, it sends the video server a message with the new information. This could be the addition or removal of camera feeds as well as updates to the visible objects and QoS values in a camera feed. All messages follow a similar format:

    <timestamp>\n
    \n
    [feeds]

where feeds is one of
<camera id>;<camera url>;[objects/QoS values]\n
DELETE <camera id>;<camera url>;\n

<timestamp> is a string in the form 'DD-Mon-YY HH:MM:SS'
<camera id> is an integer key for the camera
<camera url> is the URL for the raw camera feed
[objects\QoS values] is an optional list of [object id];[QoS value]; pairs for visible objects where [object id] is an integer and [QoS value] is a floating point number.

If a feed starts with the DELETE keyword, that means we should stop streaming that camera feed. Otherwise, it is either an add or update command. If we are already streaming the camera feed, we assume it just contains an update to the objects and QoS values in the camera's view. If we are not streaming that camera's feed, we assume it is a new camera added to the system. The feeds section may consist of zero or more such feed statements, and the different types of feed statements (add/update/delete) may be mixed in any order.

If any invalid messages are received from the QoS server that are not in this format, we simply discard them and wait for the next message to arrive. Invalid messages do not affect the state of the QoS server.


Output:

The video delivery server publishes URLs for live streams to the Camera? table in the client group's database, and the URLs for archive videos in the Archive? table in the client group's database.

= Testing ======================================================================

Goals for testing

One goal for testing was to be able to test the video delivery server independently of the other groups' servers. We needed a way to ensure that our code functioned properly so that we could be reasonably confident that it will work when connected to other parts of the system. To do this, we created a simulator for the QoS server, which is where we receive our input.

Another goal was to be able to test our server with or without cameras connected to the network. Since we are using a third-party component to do the actual video encoding and streaming, we needed to be able to distinguish between errors in our code and errors in the third-party software. To do this we have a debug mode that does not actually launch

Testing without actual video feeds/server connections

For testing without video feeds, we replaced launching FFserver/FFmpeg processes with launching dummy processes which were programs that ran but didn't do anything. This occurs when the DEBUG setting in settings.py is set to True. When we have camera feeds available, we simply go back to using the FFserver/FFmpeg processes. The arguments given to these programs are first determined independently of our program, so they are expected to work when included in our program. The processes are called in an identical, so the choice of whether to use the actual programs or dummy programs depends only on the DEBUG setting. For capturing archive clips and thumbnails, we just skip the process creation entirely when DEBUG is set to True. Dummy processes are useful because we can run the Unix `ps` command to check to see that the processes were actually launched and running.

Unit tests for each module

We are working on writing unit tests for each module with the goal of achieveing as close to 100% code coverage as possible. An obvious reason is to verify that each of our code modules works correctly (at least for the test cases given). Another less obvious reason, is because Python is a dynamic language, we don't want to run into a situation where we are depending to perform an operation on an unsupported type or attempting to use an unbound variable, which would raise a ValueError (possibly because of a misspelling of the variable name) because that line has not yet been executed.

Server simulator

The server simulator is a separate process that our program can connect via a socket in an identical manner to the actual QoS server. The server simulator then sends messages to our video delivery server at a constant rate (e.g. one message per second). The messages are specified in a test script stored in the Server/VideoDelivery/tests/ directory. The server simulator reads this script and prepends a timestamp to each message and sends it to the video delivery server in an identical format to those messages sent by the actual QoS server. The log and transcript file for the video delivery can then be examined to determine if the video delivery server responded appropriately to each messages. When the server simulator reaches the end of a test script it shuts down. The video delivery should remain running and reconnect if the server simulator is run again. This allows us to test that the video delivery server responds appropriately to lost connections and also allows us to run multiple tests sequentially without the need to restart the video delivery server each time.

Testing in this manner has several advantages:

1) It decouples the tests from the actual running of the tests by using test scripts. This allows us to write new and diverse test cases that do not require us to modify the server simulator's code.

2) If we have camera feeds available, we can use a test script that includes the actual URLs of those camera feeds and the video delivery server will transcode and stream the video from these cameras.

3) We can create malformed messages to make sure the video delivery server recovers appropriately from unexpected input.


= Performance ==================================================================

[TODO] We should do some more testing before the next demo, and try and take some quantifiable measurements.
 
